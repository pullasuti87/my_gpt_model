# MY STEPS

1. download training data/dataset
2. define parameters
3. encode/decode data
4. split data to training and validation
5. define model
6. train, run, check the results

## claude 3.5 steps

- setup

>-- import libraries
>-- define hyperparameters
>-- set device (CPU/GPU)

- data processing

>-- load text file
>-- create character vocabulary
>-- build encoding/decoding mappings
>-- split data into train/val

- data loading

>-- implement batch generation
>-- add device handling

- model architecture

>--  build attention mechanism
>--  create transformer blocks
>--  implement BigramLanguageModel
>--  add generation capability

- training Loop

>--  initialize model and optimizer
>--  create training iterations
>--  add loss evaluation
>--  implement gradient updates

- generation

>--  create context tensor
>--  generate new tokens
>--  decode output
